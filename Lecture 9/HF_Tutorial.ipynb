{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hugging Face Transformers Tutorial\n",
    "\n",
    "\n",
    "check Hugging Face Website for more details: https://huggingface.co/docs/transformers/index\n",
    "\n",
    "It is adviced to run this notebook in Google Colab since it provides free GPU access.\n"
   ],
   "id": "1cb9e2b37e785b69"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ],
   "id": "be6191c3e2dd0572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ],
   "id": "11d486f5af3644c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")"
   ],
   "id": "8662468dfea02283"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = \"I'm excited to learn about Hugging Face Transformers!\"\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = model(**tokenized_inputs)\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "prediction = torch.argmax(outputs.logits)\n",
    "\n",
    "\n",
    "print(\"Input:\")\n",
    "print(inputs)\n",
    "print()\n",
    "print(\"Tokenized Inputs:\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "print(\"Model Outputs:\")\n",
    "print(outputs)\n",
    "print()\n",
    "print(f\"The prediction is {labels[prediction]}\")"
   ],
   "id": "f993ecc47eecb98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0.1 Tokenizers",
   "id": "cc632fd93382e035"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")      # written in Python\n",
    "print(tokenizer)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\")  # written in Rust\n",
    "print(tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\") # convenient! Defaults to Fast\n",
    "print(tokenizer)"
   ],
   "id": "79cc4998729cfb5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# This is how you call the tokenizer\n",
    "input_str = \"Hugging Face Transformers is great!\"\n",
    "tokenized_inputs = tokenizer(input_str)\n",
    "\n",
    "\n",
    "print(\"Vanilla Tokenization\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "\n",
    "# Two ways to access:\n",
    "print(tokenized_inputs.input_ids)\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ],
   "id": "8349e863d6df78c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cls = [tokenizer.cls_token_id]\n",
    "sep = [tokenizer.sep_token_id]\n",
    "\n",
    "# Tokenization happens in a few steps:\n",
    "input_tokens = tokenizer.tokenize(input_str)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "input_ids_special_tokens = cls + input_ids + sep\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
    "\n",
    "print(\"start:                \", input_str)\n",
    "print(\"tokenize:             \", input_tokens)\n",
    "print(\"convert_tokens_to_ids:\", input_ids)\n",
    "print(\"add special tokens:   \", input_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decode:               \", decoded_str)\n",
    "\n",
    "# NOTE that these steps don't create the attention mask or add the special characters"
   ],
   "id": "14d68ac3b3d7e92c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For Fast Tokenizers, there's another option too:\n",
    "inputs = tokenizer._tokenizer.encode(input_str)\n",
    "\n",
    "print(input_str)\n",
    "print(\"-\"*5)\n",
    "print(f\"Number of tokens: {len(inputs)}\")\n",
    "print(f\"Ids: {inputs.ids}\")\n",
    "print(f\"Tokens: {inputs.tokens}\")\n",
    "print(f\"Special tokens mask: {inputs.special_tokens_mask}\")\n",
    "print()\n",
    "print(\"char_to_word gives the wordpiece of a character in the input\")\n",
    "char_idx = 8\n",
    "print(f\"For example, the {char_idx + 1}th character of the string is '{input_str[char_idx]}',\"+\\\n",
    "      f\" and it's part of wordpiece {inputs.char_to_token(char_idx)}, '{inputs.tokens[inputs.char_to_token(char_idx)]}'\")"
   ],
   "id": "10172c416cdf03c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Other cool tricks:\n",
    "# The tokenizer can return pytorch tensors\n",
    "model_inputs = tokenizer(\"Hugging Face Transformers is great!\", return_tensors=\"pt\")\n",
    "print(\"PyTorch Tensors:\")\n",
    "print_encoding(model_inputs)"
   ],
   "id": "79e7746856f98b11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You can pass multiple strings into the tokenizer and pad them as you need\n",
    "model_inputs = tokenizer([\"Hugging Face Transformers is great!\",\n",
    "                         \"The quick brown fox jumps over the lazy dog.\" +\\\n",
    "                         \"Then the dog got up and ran away because she didn't like foxes.\",\n",
    "                         ],\n",
    "                         return_tensors=\"pt\",\n",
    "                         padding=True,\n",
    "                         truncation=True)\n",
    "print(f\"Pad token: {tokenizer.pad_token} | Pad token id: {tokenizer.pad_token_id}\")\n",
    "print(\"Padding:\")\n",
    "print_encoding(model_inputs)"
   ],
   "id": "4b2837869334f9b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You can also decode a whole batch at once:\n",
    "print(\"Batch Decode:\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids))\n",
    "print()\n",
    "print(\"Batch Decode: (no special characters)\")\n",
    "print(tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=True))"
   ],
   "id": "225c3ff60c494af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1 Models\n",
    "\n",
    "```\n",
    "*\n",
    "*ForMaskedLM\n",
    "*ForSequenceClassification\n",
    "*ForTokenClassification\n",
    "*ForQuestionAnswering\n",
    "*ForMultipleChoice\n",
    "...\n",
    "```\n",
    "where `*` can be `AutoModel` or a specific pretrained model (e.g. `DistilBert`)\n",
    "\n",
    "\n",
    "There are three types of models:\n",
    "* Encoders (e.g. BERT)\n",
    "* Decoders (e.g. GPT2)\n",
    "* Encoder-Decoder models (e.g. BART or T5)"
   ],
   "id": "e55961a6b7d92b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForSequenceClassification, DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=2)"
   ],
   "id": "9f2b0c8c41022379"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "# Option 1\n",
    "model_outputs = model(input_ids=model_inputs.input_ids, attention_mask=model_inputs.attention_mask)\n",
    "\n",
    "# Option 2 - the keys of the dictionary the tokenizer returns are the same as the keyword arguments\n",
    "#            the model expects\n",
    "\n",
    "# f({k1: v1, k2: v2}) = f(k1=v1, k2=v2)\n",
    "\n",
    "model_outputs = model(**model_inputs)\n",
    "\n",
    "print(model_inputs)\n",
    "print()\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Distribution over labels: {torch.softmax(model_outputs.logits, dim=1)}\")"
   ],
   "id": "cc1df1f59f1232ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You can calculate the loss like normal\n",
    "label = torch.tensor([1])\n",
    "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "# You can get the parameters\n",
    "list(model.named_parameters())[0]"
   ],
   "id": "a54a728e3187b1c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hugging Face provides an additional easy way to calculate the loss as well:",
   "id": "f2ed31b41b56a579"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# To calculate the loss, we need to pass in a label:\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "\n",
    "labels = ['NEGATIVE', 'POSITIVE']\n",
    "model_inputs['labels'] = torch.tensor([1])\n",
    "\n",
    "model_outputs = model(**model_inputs)\n",
    "\n",
    "\n",
    "print(model_outputs)\n",
    "print()\n",
    "print(f\"Model predictions: {labels[model_outputs.logits.argmax()]}\")"
   ],
   "id": "b7e3e450789e951a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-cased\", output_attentions=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    model_output = model(**model_inputs)\n",
    "\n",
    "\n",
    "print(\"Hidden state size (per layer):  \", model_output.hidden_states[0].shape)\n",
    "print(\"Attention head size (per layer):\", model_output.attentions[0].shape)     # (layer, batch, query_word_idx, key_word_idxs)\n",
    "                                                                               # y-axis is query, x-axis is key\n",
    "print(model_output)"
   ],
   "id": "fe23af9e407cb10c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
