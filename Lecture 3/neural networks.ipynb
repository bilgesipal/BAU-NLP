{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "183937cc029a129d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Massive Text Embedding Benchmark\n",
    "\n",
    "\n",
    "https://huggingface.co/datasets/mteb/tweet_sentiment_extraction"
   ],
   "id": "53bbae43f0926ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"mteb/tweet_sentiment_extraction\")"
   ],
   "id": "bf67e687383ea3cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ds",
   "id": "411f085b67b04690",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Explore the dataset\n",
    "ds['train'][0]"
   ],
   "id": "202b546ba97fa3a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_map = {str(lbl): lbl_txt for lbl, lbl_txt in zip(ds['train']['label'], ds['train']['label_text'])}\n",
    "label_map"
   ],
   "id": "edcfba55e1709dad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training a SentencePiece Tokenizer",
   "id": "6e07bcc0292407da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in ds[\"train\"]:\n",
    "        words = line['text']\n",
    "        f.write(words + \"\\n\")"
   ],
   "id": "952312c5d264143b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "options = dict(\n",
    "  input=\"corpus.txt\",\n",
    "  input_format=\"text\",\n",
    "  model_prefix=\"simple_nn_tok\",\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=2048,\n",
    "  byte_fallback=True,\n",
    "  num_threads=os.cpu_count()\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options);\n",
    "\n"
   ],
   "id": "e5e1081d3505affa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('simple_nn_tok.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab[1000:1020]"
   ],
   "id": "596239671c43224f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5d6e4416cc0f5900",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Test Split",
   "id": "7a22c6e0047d9ac7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_text = ds['train']['text'][:-1000]\n",
    "train_labels = ds['train']['label'][-1000:]\n",
    "val_text= ds['train']['text'][1000:]\n",
    "val_labels = ds['train']['label'][1000:]"
   ],
   "id": "9efb884483e6bd9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_train_text = [sp.encode(text) for text in train_text]\n",
    "tokenized_val_text = [sp.encode(text) for text in val_text]"
   ],
   "id": "a50302390556a7ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nwords = len(sp)\n",
    "ntags = 3 # Keep in mind that we have 3 different labels: 0, 1, 2"
   ],
   "id": "912f1c341765c003",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create the Embedding Layer\n",
    "\n",
    "First we will see the one hot encoding Then we will create the embedding layer"
   ],
   "id": "de1e724e604a2461"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install torch",
   "id": "a652249d54af0dab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import torch",
   "id": "c27b738858b09e2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(tokenized_train_text[0])",
   "id": "220034f5b84758df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "x = torch.tensor(tokenized_train_text[0], dtype=torch.long)  # (T,)\n",
    "one_hot = F.one_hot(x, num_classes=nwords).float()            # (T, nwords)\n",
    "print(one_hot)"
   ],
   "id": "dfdf45c2f72cf0fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "weight = nn.Parameter(torch.randn(nwords, 64))\n",
    "weight"
   ],
   "id": "3fd425fea626a469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "weight.shape",
   "id": "6dcc3d6b558f950a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "xs = F.one_hot(x, num_classes=nwords)[:5]",
   "id": "3495e997a196bab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "xs is a vector of token IDs like [273, 1989, 1974, 356, 339, ...] and nwords is the vocabulary size (say 2000),",
   "id": "7ce310607f65d266"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "xs.shape",
   "id": "85c019fed9a10eff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.matmul(xs.float(), weight).shape",
   "id": "8d519e1affb97ec9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Equivalent (and faster) form\n",
    "\n",
    "You can skip the one-hot and directly use nn.Embedding:"
   ],
   "id": "9394dd8e5b0bff3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=nwords, embedding_dim=64)\n",
    "out = embedding(x[:5])\n",
    "print(out.shape)  # torch.Size([5, 64])\n"
   ],
   "id": "9f8487f74c11cbfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Simple_NN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super(Simple_NN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, num_labels)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        emb = self.embedding(tokens)\n",
    "        out = torch.sum(emb, dim=0)\n",
    "        logits = out.view(1, -1)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "e7782472c6f28f36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Cross Entropy Loss Function\n",
    "def ce_loss(logits, target):\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "    loss = -log_probs[:, target]\n",
    "    return loss"
   ],
   "id": "a63e6561c903ad46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can use the SGD (Stochastic Gradient Descent) optimizer that was introduced in class, or this typically better optimizer Adam (we'll see it in a later class).",
   "id": "e3a09be9a088d64e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data = list(zip(tokenized_train_text, train_labels))\n",
    "val_data   = list(zip(tokenized_val_text,   val_labels))"
   ],
   "id": "3cbae80333d9978b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# initialize the model\n",
    "model = Simple_NN(nwords, ntags)\n",
    "criterion = ce_loss\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    random.shuffle(train_data)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for x, y in train_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                ITER, train_loss/len(train_data), time.time()-start))\n",
    "    # Perform validation\n",
    "    test_correct = 0.0\n",
    "    for x, y in val_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: valid acc=%.4f\" % (ITER, test_correct/len(val_data)))\n",
    "\n"
   ],
   "id": "7273c4ac4248a06a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ],
   "id": "752329efc2c2756d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data=list(zip([enc.encode(text) for text in ds['train']['text']], ds['train']['label']))",
   "id": "e15f0386485e6c92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
   ],
   "id": "90d1a215e4ede5de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nwords = nwords = enc.n_vocab\n",
    "ntags = 3 # Keep in mind that we have 3 different labels: 0, 1, 2\n"
   ],
   "id": "f312be5fffeb848a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# initialize the model\n",
    "model = Simple_NN(nwords, ntags)\n",
    "criterion = ce_loss\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for ITER in range(5):\n",
    "    # Perform training\n",
    "    random.shuffle(train_data)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    for x, y in train_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                ITER, train_loss/len(train_data), time.time()-start))\n",
    "    # Perform validation\n",
    "    test_correct = 0.0\n",
    "    for x, y in val_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: valid acc=%.4f\" % (ITER, test_correct/len(val_data)))"
   ],
   "id": "1b94da4ed71b5f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CBOW model for word2vec",
   "id": "e384ab335fb8350c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CBoW(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, emb_size):\n",
    "        super(CBoW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.output_layer = nn.Linear(emb_size, num_labels)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        emb = self.embedding(tokens)    # [len(tokens) x emb_size]\n",
    "        emb_sum = torch.sum(emb, dim=0) # [emb_size]\n",
    "        h = emb_sum.view(1, -1)         # [1 x emb_size]\n",
    "        logits = self.output_layer(h)   # [1 x num_labels]\n",
    "        return logits"
   ],
   "id": "3601f0ed8d727905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EMB_SIZE=32\n",
    "model = CBoW(nwords, ntags, EMB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "for ITER in range(5):\n",
    "    random.shuffle(train_data)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    for x, y in train_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor([y])\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % (\n",
    "                ITER, train_loss/len(train_data), time.time()-start))\n",
    "    model.eval()\n",
    "    # Perform testing\n",
    "    test_correct = 0.0\n",
    "    for x, y in val_data:\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        logits = model(x)[0].detach()\n",
    "        predict = logits.argmax().item()\n",
    "        if predict == y:\n",
    "            test_correct += 1\n",
    "    print(\"iter %r: validation acc=%.4f\" % (ITER, test_correct/len(val_data)))"
   ],
   "id": "35c05260d373cc7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tweet = \"I hate NLP!\"\n",
    "tokens = torch.tensor(enc.encode(tweet), dtype=torch.long)\n",
    "logits = model(tokens)[0].detach()\n",
    "predict = logits.argmax().item()\n",
    "predict"
   ],
   "id": "f4d5909b313c4334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "label_map[str(predict)]",
   "id": "9ca8bfb01c096b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualizing embeddings (fixed, PyTorch + tiktoken)\n",
    "\n",
    "This will:\n",
    "\n",
    "Grab the trained embedding matrix from the PyTorch model,\n",
    "\n",
    "PCA to 2D,\n",
    "\n",
    "Plot the most frequent tokens found in your training data (to keep the plot readable),\n",
    "\n",
    "Decode tokens with tiktoken (note: GPT-2 BPE tokens are often subword pieces; many start with a leading space)."
   ],
   "id": "c226e01ca7e9e501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "9611eb3d15397fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math, string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# --- define stopword set and helper function ---\n",
    "stop_words = {\n",
    "    \"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"in\",\"on\",\"at\",\"for\",\"is\",\"it\",\"this\",\n",
    "    \"that\",\"with\",\"as\",\"by\",\"from\",\"are\",\"was\",\"be\",\"were\",\"so\",\"if\",\"but\",\"not\",\n",
    "    \"i\",\"you\",\"he\",\"she\",\"we\",\"they\",\"me\",\"my\",\"your\",\"our\",\"their\", \"good\", \"too\", \"like\"\n",
    "}\n",
    "\n",
    "def is_meaningful(tok):\n",
    "    \"\"\"Return True if token is not punctuation/stopword/empty.\"\"\"\n",
    "    text = enc.decode([tok]).strip().lower()\n",
    "    if not text or all(ch in string.punctuation for ch in text):\n",
    "        return False\n",
    "    if text in stop_words:\n",
    "        return False\n",
    "    return True"
   ],
   "id": "16b6d5e35dcae4e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Pure PyTorch PCA + Matplotlib (no .numpy())\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import math\n",
    "from collections import defaultdict\n",
    "freq = Counter()\n",
    "for x_ids, _ in train_data:\n",
    "    freq.update(x_ids)\n",
    "\n",
    "# take top-N among most-common after simple filter\n",
    "N = 30\n",
    "top_tokens = [tok for tok, _ in freq.most_common(1000) if is_meaningful(tok)][:N]\n",
    "\n",
    "# 2) PCA with pure torch (no numpy/sklearn)\n",
    "with torch.no_grad():\n",
    "    E = model.embedding.weight.detach().cpu()            # [vocab, emb_dim]\n",
    "    E_centered = E - E.mean(dim=0, keepdim=True)\n",
    "    U, S, V = torch.pca_lowrank(E_centered, q=2)\n",
    "    reduced = E_centered @ V[:, :2]                      # [vocab, 2]\n",
    "\n",
    "# 3) plot\n",
    "xs = reduced[top_tokens, 0].tolist()\n",
    "ys = reduced[top_tokens, 1].tolist()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(xs, ys, s=10)\n",
    "\n",
    "for tok, x, y in zip(top_tokens, xs, ys):\n",
    "    label = enc.decode([tok]).replace(\"\\n\", \" \").strip() or f\"␀{tok}\"\n",
    "    plt.annotate(label, xy=(x, y), xytext=(3, 2), textcoords=\"offset points\")\n",
    "\n",
    "plt.title(\"CBOW Embeddings (Top Non-Stopword Tokens)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d1778294d653cbd4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
