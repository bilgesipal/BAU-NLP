{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory Data Analysis",
   "id": "3681ac3309b202f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Get necessary packages and Data\n",
    "Check your data"
   ],
   "id": "ca23faa75156110"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-14T10:23:19.042096Z",
     "start_time": "2025-02-14T10:23:16.604471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "id": "a3bc01c940516930",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bilgesipal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bilgesipal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the Data\n",
    "with open('../df_data.pkl', \"rb\") as f:\n",
    "    df_data = pickle.load(f)\n",
    "print(len(df_data))\n",
    "\n",
    "with open('../label.pkl', \"rb\") as f:\n",
    "    label = pickle.load(f)"
   ],
   "id": "e3c5d1ed8bc8bece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('type of the data:', type(df_data))\n",
    "print('size of the data:', len(df_data))"
   ],
   "id": "3cd4d9316852cbd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_data[:2]",
   "id": "6dfdccfd74231ac2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "str(label[0])",
   "id": "f560dc700a9c0e87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_list = label.tolist()\n",
    "label = [str(i) for i in label_list]"
   ],
   "id": "75b38b7ebd3d618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert the Corpus to a Data Frame",
   "id": "b5751614c0a5bfbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create list of dictionaries for each entry\n",
    "stp_corpus = [{'data': data, 'label': label} for data, label in zip(df_data, label_list)]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(stp_corpus)\n",
    "\n",
    "df.head()\n"
   ],
   "id": "31986d6c2a3ba0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = df['data'].iloc[0]\n",
    "data"
   ],
   "id": "2e11d361a8980308",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clean the Data\n",
    "\n",
    "Text data cleaning, also known as text pre-processing, is essential for improving model performance. Since text cleaning can be an endless process, we will start simple and refining iteratively.\n",
    "Basic Cleaning Steps:\n",
    "\n",
    "    Convert text to lowercase\n",
    "    Remove punctuation and numbers\n",
    "    Eliminate non-sensical text (e.g., \\n)\n",
    "    Tokenize text\n",
    "    Remove stop words\n",
    "\n",
    "Advanced Cleaning After Tokenization:\n",
    "\n",
    "    Stemming/Lemmatization\n",
    "    POS tagging\n",
    "    Bigram/Trigram creation\n",
    "    Handling typos\n",
    "\n",
    "More refinements can be applied later to enhance results.\n",
    "\n",
    "\n"
   ],
   "id": "181ae243b44b2364"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Examples --> Playground",
   "id": "8f0c16c5771142dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Original text\n",
    "text = \"\"\"From: bil@okcforum.osrhe.edu (Bill Conner)\\nSubject: Re: Not the Omni!\\nNntp-Posting-Host: okcforum.osrhe.edu\\nOrganization: Okcforum Unix Users Group\\nX-Newsreader: TIN [version 1.1 PL6]\\nLines: 18\\n\\nCharley Wingate (mangoe@cs.umd.edu) wrote:\\n: \\n: >> Please enlighten me.  How is omnipotence contradictory?\\n: \\n: >By definition, all that can occur in the universe is governed by the rules\\n: >of nature. Thus god cannot break them. Anything that god does must be allowed\\n: >in the rules somewhere. Therefore, omnipotence CANNOT exist! It contradicts\\n: >the rules of nature.\\n: \\n: Obviously, an omnipotent god can change the rules.\\n\\nWhen you say, \"By definition\", what exactly is being defined;\\ncertainly not omnipotence. You seem to be saying that the \"rules of\\nnature\" are pre-existant somehow, that they not only define nature but\\nactually cause it. If that\\'s what you mean I\\'d like to hear your\\nfurther thoughts on the question.\\n\\nBill\\n\"\"\"\n",
    "\n",
    "#  Remove email headers\n",
    "cleaned_text = re.sub(r\"^(From|Subject|Nntp-Posting-Host|Organization|X-Newsreader|Lines):.*\\n?\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "# Remove email addresses\n",
    "cleaned_text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"\", cleaned_text)\n",
    "\n",
    " #  Remove all < and > characters\n",
    "cleaned_text = re.sub(r\"[<>]\", \"\", cleaned_text)\n",
    "\n",
    " # Remove extra colons and quote symbols (:, >)\n",
    "cleaned_text = re.sub(r\"^[:>\\s]+\", \"\", cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "# Remove all newlines and replace with a space\n",
    "#This line of code removes extra newlines (\\n) and replaces them with a single space while also trimming any leading or trailing spaces.\n",
    "cleaned_text = re.sub(r\"\\n+\", \" \", cleaned_text).strip()\n",
    "\n",
    "# Display cleaned text\n",
    "print(cleaned_text)\n"
   ],
   "id": "6ff9152ec81dab1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doc = nlp(cleaned_text)",
   "id": "e7adda8a0bee7489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Extract lemmatized text (excluding stopwords and punctuation)\n",
    "lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "# Display cleaned, lemmatized, and stopword-removed text\n",
    "print(lemmatized_text)"
   ],
   "id": "3da21cb5fa5a3b30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(lemmatized_text)",
   "id": "7d792e4eaccfb6db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Pickle files for later use\n",
    "\n",
    "#     with open(\"data.txt\", \"wb\") as file:\n",
    "#         pickle.dump(transcripts[i], file)"
   ],
   "id": "6a097c97f9fad53c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The Cleaner Functions",
   "id": "69317bf48b75b188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by performing various preprocessing steps:\n",
    "\n",
    "    1. **Removes Metadata Headers**: Deletes lines that start with\n",
    "       'From', 'Subject', 'Nntp-Posting-Host', 'Organization',\n",
    "       'X-Newsreader', or 'Lines'.\n",
    "    2. **Removes Email Addresses**: Matches and removes any email-like patterns.\n",
    "    3. **Removes Quotation Symbols and Leading Spaces**: Removes `:` and `>`\n",
    "       characters at the beginning of lines.\n",
    "    4. **Removes Digits**: Eliminates all numeric characters.\n",
    "    5. **Removes Extra Newlines**: Replaces multiple newlines with a single space.\n",
    "    6. **Removes Angle Brackets (`< >`)**: Deletes any occurrences of `<` and `>`.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text with unwanted elements removed.\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub(r\"^(From|Subject|Nntp-Posting-Host|Organization|X-Newsreader|Lines):.*\\n?\", \"\", text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"^[:>\\s]+\", \"\", cleaned_text, flags=re.MULTILINE)\n",
    "    cleaned_text = re.sub(r\"\\d+\", \"\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\n+\", \" \", cleaned_text).strip()\n",
    "    cleaned_text = re.sub(r\"[<>]\", \"\", cleaned_text)\n",
    "\n",
    "    return cleaned_text\n"
   ],
   "id": "ddb4e1fb98608b71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lemmatizer(cleaned_text):\n",
    "    \"\"\"\n",
    "    Applies lemmatization to the input text using SpaCy, while also removing stop words and punctuation.\n",
    "\n",
    "    Steps:\n",
    "    1. **Tokenizes** the input text using a SpaCy NLP model.\n",
    "    2. **Lemmatizes** each token (converts words to their base form).\n",
    "    3. **Removes Stop Words** (e.g., \"the\", \"is\") to retain only meaningful words.\n",
    "    4. **Removes Punctuation** to clean the text further.\n",
    "    5. **Joins the lemmatized words** back into a single string.\n",
    "\n",
    "    Args:\n",
    "        cleaned_text (str): The input text that has already been preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized text with stop words and punctuation removed.\n",
    "\n",
    "    Example:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        cleaned_text = \"The dogs are running quickly towards the park.\"\n",
    "        print(lemmatizer(cleaned_text))\n",
    "        'dog run quickly park'\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(cleaned_text)\n",
    "    lemmatized_text = \" \".join([str(token.lemma_) for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "    return lemmatized_text"
   ],
   "id": "a643add59b3ff4c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "cffb52dec69666c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['clean_text'] = df['data'].apply(lambda x: clean_text(x))",
   "id": "f1c01d14f1f29c45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.clean_text.iloc[2]",
   "id": "feb6f168a38427de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['new_data'] = df.clean_text.apply(lambda x: lemmatizer(x))",
   "id": "8744f637effb01ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../clean_df.pkl\", \"wb\") as file:\n",
    "    pickle.dump(df[:100], file)"
   ],
   "id": "3ae9ea3d2fd349b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualise --EDA",
   "id": "ae29f4caf4d97a19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### WORD CLOUDS\n",
    "\n",
    "A Word Cloud (also called a tag cloud) is a visual representation of text data, where:\n",
    "\n",
    "    More frequent words appear larger, while\n",
    "    Less frequent words appear smaller\n",
    "\n",
    "It is commonly used in NLP and text analysis to identify important words in a dataset.\n",
    "Example Use Cases\n",
    "\n",
    "    Analyzing customer reviews (e.g., finding common themes in feedback)\n",
    "    Summarizing large text data (e.g., extracting key terms from news articles)\n",
    "    Exploring social media trends (e.g., identifying frequently used words in tweets)"
   ],
   "id": "f88d116610ceadd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"../clean_df.pkl\", \"rb\") as file:\n",
    "    data_clean = pickle.load(file)"
   ],
   "id": "671de8eb46b08244",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_clean.head()",
   "id": "fc1242e7b08ab642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_clean.label.value_counts()",
   "id": "24a037235cf18449",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "grouped_df = data_clean.groupby('label')['new_data'].agg(' '.join).reset_index().copy()\n",
    "grouped_df"
   ],
   "id": "48ff7981c1b5056b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's make some word clouds!\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)"
   ],
   "id": "53b842b23c9eaa40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "full_names = ['0', '1']\n",
    "\n",
    "# Create subplots for each comedian\n",
    "for i in range(2):\n",
    "    wc.generate(grouped_df.new_data.iloc[i])\n",
    "\n",
    "\n",
    "    plt.subplot(3, 4, 2+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_names[i])\n",
    "    plt.show()"
   ],
   "id": "1c128ad176cbd8c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create document X term matrix and Do Further Analysis",
   "id": "e4f29459ecc8b8d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's create our document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer()\n",
    "data_cv = cv.fit_transform(grouped_df.new_data)\n",
    "\n"
   ],
   "id": "2072f6d1f48fe23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_new = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_new.index = data_new.index\n"
   ],
   "id": "2460270614ec6553",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_new.head()",
   "id": "371ab2eec2f93b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_new.columns[:10]",
   "id": "2bf55c7dafe16f1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data_new['abbreviations'].sort_values(ascending=False)",
   "id": "3d91d94b4413b413",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = data_new.transpose()\n",
    "data.head()\n"
   ],
   "id": "cea4b2012c41602c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the top 10 words in each topic\n",
    "top_dict = {}\n",
    "for c in data:\n",
    "    top = data[c].sort_values(ascending=False).head(10)\n",
    "    top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ],
   "id": "a4f16a47fda35a51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print the top 10 words per topic\n",
    "for topic, top_words in top_dict.items():\n",
    "    print(topic)\n",
    "    print(', '.join([word for word, count in top_words[0:10]]))\n",
    "    print('---')"
   ],
   "id": "686d368f4862e930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Find the number of unique words that each topic has\n",
    "\n",
    "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
    "unique_list = []\n",
    "for comedian in data.columns:\n",
    "    uniques = data[comedian].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "# Create a new dataframe that contains this unique word count\n",
    "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['topic', 'unique_words'])\n",
    "data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort"
   ],
   "id": "f9fd461f702fb88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the words per mail of each topic\n",
    "\n",
    "# Find the total number of words that topic has\n",
    "total_list = []\n",
    "for comedian in data.columns:\n",
    "    totals = sum(data[comedian])\n",
    "    total_list.append(totals)\n",
    "\n",
    "#\n",
    "mails = [46, 54]\n",
    "\n",
    "# Let's add some columns to our dataframe\n",
    "data_words['total_words'] = total_list\n",
    "data_words['mail_number'] = mails\n",
    "data_words['words_per_mail'] = data_words['total_words'] / data_words['mail_number']\n",
    "\n",
    "# Sort the dataframe by words per mail, to see which mails are more verbose\n",
    "data_wpm_sort = data_words.sort_values(by='words_per_mail')\n",
    "data_wpm_sort"
   ],
   "id": "78ec1c5385237fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a77104dedd6276dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b95102653839a3e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b29feb4db1b46164",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
