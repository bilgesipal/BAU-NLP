{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hugging Face Transformers Tutorial\n",
    "\n",
    "\n",
    "check Hugging Face Website for more details: https://huggingface.co/docs/transformers/index\n",
    "\n",
    "It is recommended to run this notebook in Google Colab since it provides free GPU access.\n"
   ],
   "id": "d160e2faa10cc475"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install datasets\n",
    "!pip install -U \"transformers>=4.40.0\""
   ],
   "id": "91fe501a7ceaa656"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "import inspect, os\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers module path:\", transformers.__file__)\n",
    "print(\"TrainingArguments module:\", TrainingArguments.__module__)\n",
    "print(\"TrainingArguments signature:\", inspect.signature(TrainingArguments.__init__))"
   ],
   "id": "821726ed589fcb29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def print_encoding(model_inputs, indent=4):\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ],
   "id": "836977e2ddee769b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 2: Finetuning",
   "id": "8cfc7d7642ccf558"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.1 Loading in a dataset",
   "id": "4d0cb34f6973b6d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Just take the first 50 tokens for speed\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "# Create a small dataset\n",
    "small_imdb_dataset = DatasetDict(\n",
    "    train=imdb_dataset['train']\n",
    "        .shuffle(seed=1111)\n",
    "        .select(range(128))\n",
    "        .map(truncate),\n",
    "\n",
    "    val=imdb_dataset['train']\n",
    "        .shuffle(seed=1111)\n",
    "        .select(range(128, 160))\n",
    "        .map(truncate),)"
   ],
   "id": "dc455722b6e594a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "small_imdb_dataset",
   "id": "17eea6a3d90ba3e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "small_imdb_dataset['train'][:10]",
   "id": "7c60ad2f23921aec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast, AutoTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ],
   "id": "234b8a6cfb856932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prepare the dataset - this tokenizes the dataset in batches of 16 examples.\n",
    "small_tokenized_dataset = small_imdb_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "small_tokenized_dataset.set_format(\"torch\")"
   ],
   "id": "5e4d3c75e7c9a3e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "small_tokenized_dataset['train'][0:2]",
   "id": "f97ae12a58308b35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_tokenized_dataset['train'], batch_size=16)\n",
    "eval_dataloader = DataLoader(small_tokenized_dataset['val'], batch_size=16)"
   ],
   "id": "6c3c85189699c640"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Training",
   "id": "7c2f346b51d4178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, DistilBertForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import torch"
   ],
   "id": "f294659a4cbc15f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ],
   "id": "cb3234f7efcaee5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)"
   ],
   "id": "61cc0fd93707e0ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss = 0\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    # ------------------------ TRAIN -----------------------\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # ---------------------- VALIDATION ---------------------\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(eval_dataloader)\n",
    "    print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        print(\"Saving checkpoint!\")\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "        }, f\"checkpoints/epoch_{epoch}.pt\")\n"
   ],
   "id": "b895bd1499bfa7f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "small_imdb_dataset = DatasetDict(\n",
    "    train=imdb_dataset['train'].shuffle(seed=1111).select(range(128)).map(truncate),\n",
    "    val=imdb_dataset['train'].shuffle(seed=1111).select(range(128, 160)).map(truncate),\n",
    ")\n",
    "\n",
    "small_tokenized_dataset = small_imdb_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], truncation=True),\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")"
   ],
   "id": "1036c03542e78d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments, Trainer, DistilBertForSequenceClassification\n",
    "import numpy as np  # make sure this is imported\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-cased',\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"sample_hf_trainer\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",          # <<< changed here\n",
    "    save_strategy=\"epoch\",          # this is fine\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=224\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": np.mean(predictions == labels)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=arguments,\n",
    "    train_dataset=small_tokenized_dataset['train'],\n",
    "    eval_dataset=small_tokenized_dataset['val'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "44918d58982620d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import TrainerCallback, EarlyStoppingCallback\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        # ensure directory exists\n",
    "        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is None:\n",
    "            return\n",
    "\n",
    "        # remove very large unnecessary field\n",
    "        logs.pop(\"total_flos\", None)\n",
    "\n",
    "        # only the main process writes (important in distributed setups)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n"
   ],
   "id": "5384608355c9c873"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "id": "6ad18b16c2b7f864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.integrations import WandbCallback, TensorBoardCallback\n",
    "\n",
    "# remove the built-in ones, if present\n",
    "trainer.remove_callback(EarlyStoppingCallback)\n",
    "trainer.remove_callback(WandbCallback)\n",
    "trainer.remove_callback(TensorBoardCallback)\n",
    "\n",
    "# now add your own ones\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "trainer.add_callback(LoggingCallback(\"sample_hf_trainer/log.jsonl\"))"
   ],
   "id": "d111bd3465af5fa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# train the model\n",
    "trainer.train()"
   ],
   "id": "e15f8a7229727781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# evaluating the model is very easy\n",
    "\n",
    "# results = trainer.evaluate()                           # just gets evaluation metrics\n",
    "results = trainer.predict(small_tokenized_dataset['val']) # also gives you predictions"
   ],
   "id": "55ee36a063800c4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "results",
   "id": "b37c58051c1a9bd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\n",
    "test_str = \"I enjoyed the movie!\"\n",
    "\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"sample_hf_trainer/checkpoint-24\")\n",
    "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ],
   "id": "877e6c21f20df63f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Included here are also some practical tips for fine-tuning:\n",
    "\n",
    "**Good default hyperparameters.**\n",
    "\n",
    "* Epochs: {2, 3, 4} (larger amounts of data need fewer epochs)\n",
    "* Batch size (bigger is better: as large as you can make it)\n",
    "* Optimizer: AdamW\n",
    "* AdamW learning rate: {2e-5, 5e-5}\n",
    "* Learning rate scheduler: linear warm up for first {0, 100, 500} steps of training\n",
    "* weight_decay (l2 regularization): {0, 0.01, 0.1}\n"
   ],
   "id": "8994e1528206db7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 3:  Generation",
   "id": "8701219b582d7a6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "gpt2.config.pad_token_id = gpt2.config.eos_token_id  # Prevents warning during decoding"
   ],
   "id": "6504c897ba215f40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"Once upon a time\"\n",
    "\n",
    "tokenized_prompt = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt2.generate(**tokenized_prompt,\n",
    "                  max_length=50,\n",
    "                  do_sample=True,\n",
    "                  top_p=0.9)\n",
    "\n",
    "    print(f\"{i + 1}) {gpt2_tokenizer.batch_decode(output)[0]}\")"
   ],
   "id": "f5d11a1fd97ae5d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining Custom Datasets",
   "id": "c169513bbbc39b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Option 1: Load into Hugging Face Datasets\n",
    "\n",
    "# Kaggle donwload https://www.kaggle.com/datasets/mexwell/the-e2e-challenge-dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"e2e-dataset/trainset.csv\")\n",
    "custom_dataset = Dataset.from_pandas(df)"
   ],
   "id": "6a673b6c29d45e7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class E2EDataset(Dataset):\n",
    "    \"\"\"Tokenize data when we call __getitem__\"\"\"\n",
    "    def __init__(self, path, tokenizer):\n",
    "        with open(path, newline=\"\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader) # skip the heading\n",
    "            self.data = [{\"source\": row[0], \"target\": row[1]} for row in reader]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        inputs = self.tokenizer(self.data[i]['source'])\n",
    "        labels = self.tokenizer(self.data[i]['target'])\n",
    "        inputs['labels'] = labels.input_ids\n",
    "        return inputs"
   ],
   "id": "76ac96b981149993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bart_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-base')",
   "id": "c4dfa4637b8d07b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = E2EDataset(\"e2e-dataset/trainset.csv\", bart_tokenizer)",
   "id": "d97f60028c92dddc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "src_texts = [\"This is the first test.\", \"This is the second test.\"]\n",
    "tgt_texts = [\"Target 1\", \"Target 2\"]\n",
    "\n",
    "batch = bart_tokenizer(\n",
    "    src_texts,\n",
    "    text_target=tgt_texts,\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "batch  # contains input_ids, attention_mask, labels"
   ],
   "id": "dd9e92d9c7624fbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pipelines",
   "id": "ec3be3532154cb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\")"
   ],
   "id": "e531600702cd7858"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can run the pipeline by just calling it on a string",
   "id": "40f1e99adc84ac41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sentiment_analysis(\"Hugging Face Transformers is really cool!\")",
   "id": "32de346910692cf5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Or on a list of strings:",
   "id": "9e9189e78d205e7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sentiment_analysis([\"I didn't know if I would like Hákarl, but it turned out pretty good.\",\n",
    "                    \"I didn't know if I would like Hákarl, and it was just as bad as I'd heard.\"])"
   ],
   "id": "c93ec9c6059a6b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Masked Language Modeling",
   "id": "5972325f872162dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", fast=True)\n",
    "bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "69ade2c83daec387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = \"I am [MASK] to learn about HuggingFace!\"\n",
    "model = pipeline(\"fill-mask\", \"bert-base-cased\")\n",
    "model(prompt)"
   ],
   "id": "cf95aad24904dd20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "mask_index = np.where(inputs['input_ids'] == tokenizer.mask_token_id)\n",
    "outputs = bert(**inputs)\n",
    "top_5_predictions = torch.softmax(outputs.logits[mask_index], dim=1).topk(5)\n",
    "\n",
    "print(prompt)\n",
    "for i in range(5):\n",
    "    prediction = tokenizer.decode(top_5_predictions.indices[0, i])\n",
    "    prob = top_5_predictions.values[0, i]\n",
    "    print(f\"  {i+1}) {prediction}\\t{prob:.3f}\")"
   ],
   "id": "56992e627b310e71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
