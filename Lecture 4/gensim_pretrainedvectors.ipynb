{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gensim Word2Vec Example\n",
    "Gensim is library that contains family of algorithms, using highly optimized C routines, data streaming and Pythonic interfaces.\n",
    "\n",
    "The word2vec algorithms include skip-gram and CBOW models, using either hierarchical softmax or negative sampling"
   ],
   "id": "9dbb5b744df6ba42"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install kagglehub"
   ],
   "id": "4069d9a154f932fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ],
   "id": "a933dda7ff8f545a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Go to Kaggle website and download the archive \"Reviews.csv\" from the dataset \"Amazon Fine Food Reviews\": https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews",
   "id": "95c125502dd83077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "rev = pd.read_csv(\"Reviews.csv\")\n",
    "print(rev.head())"
   ],
   "id": "a492630a2f30587f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# We create the list of the words that our corpus has",
   "id": "bd7f66b3e04f21fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus_text = 'n'.join(rev[:1000]['Text'])\n",
    "data = []\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(corpus_text):\n",
    "    temp = []\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)"
   ],
   "id": "2b7c403eeae4967f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create the Word2Vec model with Gensim",
   "id": "33b0c86c7cb47695"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's the table converted to markdown:\n",
    "\n",
    "| Parameter | Type / Default | Meaning |\n",
    "|-----------|----------------|---------|\n",
    "| sentences | list of list of str | Your training data — a list where each item is a tokenized sentence (e.g. [['I', 'love', 'NLP'], ['Word2Vec', 'is', 'cool']]). |\n",
    "| vector_size | int, default = 100 | Dimensionality of the word vectors (i.e., number of features in the embedding). Larger size captures more semantic nuance but requires more data and memory. |\n",
    "| window | int, default = 5 | Maximum distance between the target word and its surrounding context words. A larger window means a broader context. |\n",
    "| min_count | int, default = 5 | Ignores all words that appear fewer than this number of times. Helps filter out noise and rare words. |\n",
    "| sg | int, default = 0 | Defines the training algorithm:• sg = 0 → CBOW (Continuous Bag of Words) — predicts the current word from context.• sg = 1 → Skip-gram — predicts context words from the current word. Skip-gram works better with smaller datasets and rare words. |\n",
    "| epochs | int, default = 5 | Number of iterations (epochs) over the training corpus. Increasing can improve accuracy, but training takes longer. |\n",
    "| workers | int, default = 3 | Number of CPU cores to use for training (parallelization). The higher, the faster training will be. |\n",
    "| hs | int, default = 0 | If 1, hierarchical softmax is used for training; if 0, and negative > 0, then negative sampling is used instead. |\n",
    "| negative | int, default = 5 | Number of negative samples to use. Setting this to 0 disables negative sampling. Works only if hs=0. |\n",
    "| seed | int, optional | Random seed for reproducibility. |\n",
    "| alpha | float, default = 0.025 | The initial learning rate. It decreases linearly during training. |\n",
    "| min_alpha | float, default = 0.0001 | The minimum learning rate during training decay. |\n",
    "| max_vocab_size | int, optional | Limits RAM during vocabulary building — if not None, truncates the vocabulary to this size. |"
   ],
   "id": "5cf7be47867b6c7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model1 = gensim.models.Word2Vec(data, min_count = 1,vector_size = 100, window = 5, sg=0)\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, window = 5, sg = 1)"
   ],
   "id": "e4d49238901d7bab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vector = model1.wv['tuna']  # get numpy vector of a word\n",
    "\n",
    "sims = model1.wv.most_similar('tuna', topn=10)  # get other similar words\n",
    "sims"
   ],
   "id": "21cba45b7e068acd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vector = model2.wv['tuna']  # get numpy vector of a word\n",
    "\n",
    "sims = model2.wv.most_similar('tuna', topn=10)  # get other similar words\n",
    "sims"
   ],
   "id": "de8f5dd6eb6442af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The trained word vectors are stored in a KeyedVectors instance, as model.wv as seen above\n",
    "The reason for separating the trained vectors into KeyedVectors is that if you don’t need the full model state any more (don’t need to continue training), its state can be discarded, keeping just the vectors and their keys proper.\n",
    "\n",
    "This results in a much smaller and faster object that can be mmapped for lightning fast loading and sharing the vectors in RAM between processes:"
   ],
   "id": "ccab86666d015526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Store just the words + their trained embeddings.\n",
    "\n",
    "word_vectors = model1.wv\n",
    "\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "\n",
    "# Load back with memory-mapping = read-only, shared across processes.\n",
    "\n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "\n",
    "vector = wv['computer']  # Get numpy vector of a word\n",
    "vector"
   ],
   "id": "76d3b781178f4505",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code that finds the top similar words to \"tuna\" from your wv vectors and plots them in 2-D using PCA (deterministic and fast). It also highlights \"tuna\" and labels all points.",
   "id": "a3f76f74d243d588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_similar_words(wv, query=\"tuna\", topn=15):\n",
    "    # Graceful fallback for casing\n",
    "    if query not in wv.key_to_index:\n",
    "        if query.lower() in wv.key_to_index:\n",
    "            query = query.lower()\n",
    "        elif query.title() in wv.key_to_index:\n",
    "            query = query.title()\n",
    "        else:\n",
    "            raise KeyError(f\"'{query}' not in vocabulary.\")\n",
    "\n",
    "    # Get similar words\n",
    "    sims = wv.most_similar(query, topn=topn)  # [(word, score), ...]\n",
    "    words = [w for w, _ in sims] + [query]\n",
    "\n",
    "    # Collect vectors\n",
    "    X = np.vstack([wv[w] for w in words])\n",
    "\n",
    "    # 2D projection\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X2 = pca.fit_transform(X)\n",
    "\n",
    "    # Split for styling\n",
    "    query_xy = X2[-1]\n",
    "    others_xy = X2[:-1]\n",
    "    other_labels = words[:-1]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # plot similar words\n",
    "    plt.scatter(others_xy[:, 0], others_xy[:, 1], s=60, alpha=0.8)\n",
    "    # plot the query word\n",
    "    plt.scatter(query_xy[0], query_xy[1], s=150, marker='*')  # highlighted\n",
    "\n",
    "    # annotate others\n",
    "    for (x, y), label in zip(others_xy, other_labels):\n",
    "        plt.text(x + 0.02, y + 0.02, label, fontsize=10)\n",
    "\n",
    "    # annotate query last so it's on top\n",
    "    plt.text(query_xy[0] + 0.02, query_xy[1] + 0.02, query, fontsize=12, weight=\"bold\")\n",
    "\n",
    "    plt.title(f\"Top-{topn} words similar to '{query}' (PCA projection)\")\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_similar_words(wv, query=\"tuna\", topn=15)\n"
   ],
   "id": "a6de9ab9f05d682c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def bar_similarities(wv, query=\"tuna\", topn=10):\n",
    "    sims = wv.most_similar(query, topn=topn)\n",
    "    labels, scores = zip(*sims)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.barh(range(len(scores)), scores)\n",
    "    plt.yticks(range(len(scores)), labels)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Cosine similarity to '{query}'\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "bar_similarities(wv, \"cherry\")\n"
   ],
   "id": "f370e6d35fee0d33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Homework\n",
    "1. Train Word2Vec models with different parameters (e.g., vector_size, window, sg) and compare the results.\n",
    "2. Use the trained Word2Vec embeddings in a simple text classification task (e.g., sentiment analysis) and evaluate the performance.\n",
    "3. Visualize the embeddings using PCA to see how similar words cluster together."
   ],
   "id": "b87c7fafeeba91ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d60b5eec8eb8c923",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
